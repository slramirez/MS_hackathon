{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of SVM model outputs\n",
    "## Gareth Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,precision_score,recall_score,classification_report\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def performance_report(df, CIRI_label):\n",
    "    df= df[df['Class']==CIRI_label]\n",
    "    print('accuracy %s' % accuracy_score(df['y_pred'], df['y_test']))\n",
    "    print(classification_report(df['y_test'],df['y_pred']))\n",
    "    print(confusion_matrix(df['y_test'],df['y_pred']))\n",
    "    \n",
    "def generate_performance_db(df):\n",
    "    labels = df['Class'].unique()\n",
    "    df_performance =pd.DataFrame(columns = ['Class','Accuracy','Precision','Recall'])\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]     \n",
    "        df_of_int= df[df['Class']==label]\n",
    "        \n",
    "        df_performance.loc[i,'Class']     =  str(label)\n",
    "        df_performance.loc[i,'Accuracy']  =  accuracy_score(df_of_int['y_pred'], df_of_int['y_test'])\n",
    "        df_performance.loc[i,'Precision'] =  precision_score(df_of_int['y_pred'], df_of_int['y_test'],average='micro')\n",
    "        df_performance.loc[i,'Recall']    =  recall_score(df_of_int['y_test'],df_of_int['y_pred'],average='micro')\n",
    "        \n",
    "    return df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = generate_performance_db(df_model_perform)\n",
    "\n",
    "results = sd.barplot(x=performance['Class'],y=performance['Accuracy'])\n",
    "results.set_xticklabels(results.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Metric Confusion Matrix: EG PHYSINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_perform = pd.read_csv('../data/SVM_Performance.csv')\n",
    "df= df_model_perform[df_model_perform['Class']=='PHYSINT']\n",
    "array = confusion_matrix(df['y_test'],df['y_pred'])\n",
    "df_cm = pd.DataFrame(array, range(9),\n",
    "                  range(9))\n",
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 20})# font size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
